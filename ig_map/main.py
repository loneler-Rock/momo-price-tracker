# æª”æ¡ˆä½ç½®: ig_map/main.py
import os
import sys
import re
import json
import requests
from urllib.parse import unquote, quote
from bs4 import BeautifulSoup

# è¨­å®šè·¯å¾‘ä»¥å¼•ç”¨ utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.supabase_client import init_supabase

# æ¨¡æ“¬ç€è¦½å™¨ Header (é€™å¾ˆé‡è¦ï¼Œé¨™é Google æˆ‘å€‘æ˜¯é›»è…¦)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
}

def get_url_content(url):
    try:
        response = requests.get(url, headers=HEADERS, allow_redirects=True, timeout=10)
        return response.url, response.text
    except Exception as e:
        print(f"âŒ ç¶²é è®€å–å¤±æ•—: {e}")
        return url, ""

def extract_from_json_ld(soup):
    """ å¾ JSON-LD æå–åº§æ¨™ (æœ€æº–) """
    try:
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                items = data if isinstance(data, list) else [data]
                for item in items:
                    if 'geo' in item and '@type' in item:
                        lat = float(item['geo']['latitude'])
                        lng = float(item['geo']['longitude'])
                        name = item.get('name', '')
                        print(f"ğŸ’ JSON-LD å‘½ä¸­: {name}")
                        return lat, lng, name
            except: continue
    except: pass
    return None, None, None

def extract_coordinates_from_text(text):
    """
    æš´åŠ›æœå°‹ï¼šç›´æ¥åœ¨æ–‡å­—/ç¶²å€/HTMLä¸­å°‹æ‰¾åº§æ¨™ç‰¹å¾µ
    """
    # 1. Google Maps URL pattern (!3d...!4d...)
    match = re.search(r'!3d(-?\d+\.\d+)!4d(-?\d+\.\d+)', text)
    if match: return float(match.group(1)), float(match.group(2))

    # 2. @lat,long pattern
    match = re.search(r'@(-?\d+\.\d+),(-?\d+\.\d+)', text)
    if match: return float(match.group(1)), float(match.group(2))
    
    # 3. plain lat,long pattern (search param)
    match = re.search(r'q=(-?\d+\.\d+),(-?\d+\.\d+)', text)
    if match: return float(match.group(1)), float(match.group(2))

    return None, None

def extract_name_fallback(soup, url):
    """ æŠ“å–åº—å (Meta Tag æˆ– URL) """
    # 1. Meta Tag
    og_title = soup.find("meta", property="og:title")
    if og_title and og_title.get("content"):
        return og_title["content"].split('Â·')[0].strip()
    
    # 2. URL decoding
    decoded_url = unquote(url)
    match = re.search(r'/place/([^/]+)/', decoded_url)
    if match:
        return match.group(1).replace('+', ' ')
        
    return "æœªå‘½ååœ°é»"

def search_place_by_name(shop_name):
    """
    [V6.0 æ ¸å¿ƒ] å›é¦¬æ§æˆ°è¡“ï¼šç”¨åº—åå» Google Maps æœå°‹ï¼Œç²å–çœŸæ­£çš„åº§æ¨™ URL
    """
    print(f"ğŸ”„ å•Ÿå‹•å›é¦¬æ§æˆ°è¡“ï¼šæ­£åœ¨æœå°‹ã€Œ{shop_name}ã€...")
    try:
        # æ§‹é€ æœå°‹é€£çµ
        search_url = f"https://www.google.com/maps/search/{quote(shop_name)}"
        response = requests.get(search_url, headers=HEADERS, allow_redirects=True, timeout=10)
        
        print(f"ğŸ”„ æœå°‹è·³è½‰ç¶²å€: {response.url[:60]}...")
        
        # æœå°‹çµæœçš„ç¶²å€é€šå¸¸æœƒåŒ…å«åº§æ¨™
        lat, lng = extract_coordinates_from_text(response.url)
        
        # å¦‚æœç¶²å€æ²’æœ‰ï¼Œæœ HTML å…§å®¹
        if not lat:
            lat, lng = extract_coordinates_from_text(response.text)
            
        if lat and lng:
            print(f"ğŸ¯ æœå°‹æˆåŠŸï¼æ‰¾å›åº§æ¨™: {lat}, {lng}")
            return lat, lng
            
    except Exception as e:
        print(f"âš ï¸ æœå°‹å¤±æ•—: {e}")
        
    return None, None

def save_location(supabase, user_id, short_url):
    print(f"ğŸ” æ­£åœ¨è§£æ: {short_url} ...")
    
    # 1. åˆæ­¥è§£æ
    final_url, html_content = get_url_content(short_url)
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 2. å˜—è©¦ç²å–è³‡æ–™
    lat, lng, name = extract_from_json_ld(soup)
    
    # å¦‚æœ JSON-LD æ²’æŠ“åˆ°åº—åï¼Œç”¨ Meta è£œæŠ“
    if not name:
        name = extract_name_fallback(soup, final_url)
    
    print(f"ğŸ·ï¸ åµæ¸¬åº—å: {name}")

    # 3. å¦‚æœæ²’åº§æ¨™ï¼Œå…ˆè©¦è©¦çœ‹ç¶²å€/HTMLè£¡æœ‰æ²’æœ‰è—
    if not lat:
        lat, lng = extract_coordinates_from_text(final_url)
    if not lat:
        lat, lng = extract_coordinates_from_text(html_content)

    # 4. [å¤§æ‹›] å¦‚æœé‚„æ˜¯æ²’åº§æ¨™ï¼Œä½†æˆ‘å€‘æœ‰åº—å -> åŸ·è¡Œå›é¦¬æ§æœå°‹ï¼
    if (not lat or not lng) and name and name != "æœªå‘½ååœ°é»":
        print("âš ï¸ åŸå§‹é€£çµç„¡åº§æ¨™ï¼Œå˜—è©¦ä½¿ç”¨åº—ååæŸ¥...")
        lat, lng = search_place_by_name(name)

    # çµç®—
    if lat and lng:
        print(f"âœ… æœ€çµ‚é–å®š: ç·¯åº¦ {lat}, ç¶“åº¦ {lng}")
        
        data = {
            "user_id": user_id,
            "original_url": short_url,
            "name": name,
            "latitude": lat,
            "longitude": lng
        }
        
        try:
            supabase.table("ig_food_map").insert(data).execute()
            print("ğŸ‰ æˆåŠŸå„²å­˜è‡³ Supabase!")
            return True
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«å¯«å…¥å¤±æ•—: {e}")
    else:
        print("âŒ è§£æå¤±æ•—ï¼šå·²å˜—è©¦æ‰€æœ‰æ‰‹æ®µ (JSON-LD, HTML, URL, åæŸ¥æœå°‹)ï¼Œä»ç„¡æ³•ç²å–åº§æ¨™ã€‚")
    
    return False

def main():
    print("ğŸš€ IG ç¾é£Ÿåœ°åœ–è§£æå™¨ V6.0 (å›é¦¬æ§æœå°‹ç‰ˆ) å•Ÿå‹•...")
    
    if len(sys.argv) > 2:
        target_url = sys.argv[1]
        user_id = sys.argv[2]
        try:
            supabase = init_supabase()
            save_location(supabase, user_id, target_url)
        except Exception as e:
            print(f"âŒ åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}")
            sys.exit(1) 
    else:
        print("âš ï¸ ç¼ºå°‘åƒæ•¸")

if __name__ == "__main__":
    main()
