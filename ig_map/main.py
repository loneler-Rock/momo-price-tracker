# æª”æ¡ˆä½ç½®: ig_map/main.py
import os
import sys
import re
import json
import requests
import time
from urllib.parse import unquote, quote
from bs4 import BeautifulSoup

# è¨­å®šè·¯å¾‘ä»¥å¼•ç”¨ utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.supabase_client import init_supabase

# æ¨¡æ“¬ç€è¦½å™¨ Header
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
}

def get_url_content(url):
    try:
        # å¢åŠ  headers èˆ‡ timeout ç©©å®šæ€§
        response = requests.get(url, headers=HEADERS, allow_redirects=True, timeout=15)
        return response.url, response.text
    except Exception as e:
        print(f"âŒ ç¶²é è®€å–å¤±æ•—: {e}")
        return url, ""

def extract_from_json_ld(soup):
    """ [æˆ°è¡“ 1] å¾ JSON-LD æå–åº§æ¨™ (Google æ¨™æº–) """
    try:
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                items = data if isinstance(data, list) else [data]
                for item in items:
                    if 'geo' in item and '@type' in item:
                        lat = float(item['geo']['latitude'])
                        lng = float(item['geo']['longitude'])
                        name = item.get('name', '')
                        print(f"ğŸ’ JSON-LD å‘½ä¸­: {name}")
                        return lat, lng, name
            except: continue
    except: pass
    return None, None, None

def search_osm_nominatim(shop_name):
    """
    [æˆ°è¡“ 2 - æ–°åŠŸèƒ½] å‘¼å« OpenStreetMap å…è²» API æŸ¥è©¢
    é€™æ˜¯éå¸¸ä¹¾æ·¨ä¸”ç©©å®šçš„ APIï¼Œä¸éœ€çˆ¬èŸ²
    """
    print(f"ğŸŒ å‘¼å« OSM ç›Ÿè»ï¼šæŸ¥è©¢ã€Œ{shop_name}ã€...")
    try:
        # ä½¿ç”¨ Nominatim API
        base_url = "https://nominatim.openstreetmap.org/search"
        params = {
            "q": shop_name,
            "format": "json",
            "limit": 1,
            "accept-language": "zh-TW" # æŒ‡å®šä¸­æ–‡
        }
        # OSM è¦æ±‚å¿…é ˆå¸¶ User-Agent
        osm_headers = {"User-Agent": "IG_Food_Map_Bot/1.0"}
        
        response = requests.get(base_url, params=params, headers=osm_headers, timeout=10)
        data = response.json()
        
        if data and len(data) > 0:
            lat = float(data[0]['lat'])
            lng = float(data[0]['lon'])
            print(f"ğŸŒ OSM æŸ¥è©¢æˆåŠŸï¼åº§æ¨™: {lat}, {lng}")
            return lat, lng
        else:
            print("ğŸŒ OSM æŸ¥ç„¡æ­¤åœ° (å¯èƒ½æ˜¯æ–°é–‹åº—å®¶)ã€‚")
            
    except Exception as e:
        print(f"âš ï¸ OSM æŸ¥è©¢å¤±æ•—: {e}")
        
    return None, None

def extract_coordinates_brute_force(text):
    """
    [æˆ°è¡“ 3] æš´åŠ›éæ¿¾ï¼šåœ¨ HTML åŸå§‹ç¢¼ä¸­å°‹æ‰¾å°ç£ç¯„åœå…§çš„åº§æ¨™
    å°ç£ç¯„åœ: Lat 21-26, Lng 119-123
    """
    try:
        # å°‹æ‰¾æ‰€æœ‰åƒæ˜¯æµ®é»æ•¸çš„æ•¸å­—
        # Google åº§æ¨™é€šå¸¸å°æ•¸é»å¾Œæœ‰ 5-7 ä½
        candidates = re.findall(r'(\d{2,3}\.\d{4,15})', text)
        
        valid_lat = None
        valid_lng = None
        
        for num_str in candidates:
            val = float(num_str)
            # åˆ¤æ–·æ˜¯å¦ç‚ºå°ç£ç·¯åº¦ (20~27)
            if 20 < val < 27:
                valid_lat = val
            # åˆ¤æ–·æ˜¯å¦ç‚ºå°ç£ç¶“åº¦ (118~124)
            if 118 < val < 124:
                valid_lng = val
            
            # å¦‚æœæ¹Šé½Šä¸€å°ï¼Œå°±å›å‚³ (é€šå¸¸ HTML è£¡ç¶“ç·¯åº¦æœƒé å¾ˆè¿‘ï¼Œé€™å€‹ç°¡å–®é‚è¼¯é€šå¸¸æœ‰æ•ˆ)
            if valid_lat and valid_lng:
                return valid_lat, valid_lng
                
    except:
        pass
    return None, None

def extract_name_fallback(soup, url):
    """ æŠ“å–åº—å """
    # 1. Meta Tag
    og_title = soup.find("meta", property="og:title")
    if og_title and og_title.get("content"):
        return og_title["content"].split('Â·')[0].strip()
    
    # 2. HTML Title
    if soup.title:
        return soup.title.string.split(' - ')[0]
        
    return "æœªå‘½ååœ°é»"

def search_place_by_name_google(shop_name):
    """
    [æˆ°è¡“ 4] Google å›é¦¬æ§ (æ¡Œé¢ç‰ˆæœå°‹)
    """
    print(f"ğŸ”„ å•Ÿå‹• Google å›é¦¬æ§ï¼šæœå°‹ã€Œ{shop_name}ã€...")
    try:
        # å¼·åˆ¶ä½¿ç”¨æ¡Œé¢ç‰ˆæœå°‹ URL (é€™æ¯” mobile redirect ç©©å®š)
        search_url = f"https://www.google.com.tw/maps/search/{quote(shop_name)}?hl=zh-TW"
        response = requests.get(search_url, headers=HEADERS, allow_redirects=True, timeout=10)
        
        # å˜—è©¦å¾æœå°‹çµæœ HTML æš´åŠ›æŠ“åº§æ¨™
        lat, lng = extract_coordinates_brute_force(response.text)
        if lat and lng:
            print(f"ğŸ¯ Google æœå°‹ HTML æš´åŠ›ç ´è§£æˆåŠŸ: {lat}, {lng}")
            return lat, lng
            
    except Exception as e:
        print(f"âš ï¸ Google æœå°‹å¤±æ•—: {e}")
        
    return None, None

def save_location(supabase, user_id, short_url):
    print(f"ğŸ” æ­£åœ¨è§£æ: {short_url} ...")
    
    # 1. å–å¾—åˆå§‹é é¢
    final_url, html_content = get_url_content(short_url)
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # 2. å˜—è©¦ç”¨æ¨™æº–æ–¹æ³•æŠ“
    lat, lng, name = extract_from_json_ld(soup)
    
    # è£œæŠ“åº—å
    if not name:
        name = extract_name_fallback(soup, final_url)
    print(f"ğŸ·ï¸ åµæ¸¬åº—å: {name}")

    # 3. å¦‚æœæ²’åº§æ¨™ -> å‘¼å« OSM ç›Ÿè» (æœ€ä¹¾æ·¨çš„è§£æ³•)
    if (not lat or not lng) and name != "æœªå‘½ååœ°é»":
        lat, lng = search_osm_nominatim(name)

    # 4. å¦‚æœ OSM ä¹Ÿæ²’æ‰¾åˆ° -> Google æœå°‹é é¢æš´åŠ›ç ´è§£ (æœ€é«’ä½†æœ‰æ•ˆçš„è§£æ³•)
    if (not lat or not lng) and name != "æœªå‘½ååœ°é»":
        lat, lng = search_place_by_name_google(name)

    # 5. å¦‚æœé‚„æ˜¯æ²’æœ‰ -> è©¦è©¦çœ‹åŸå§‹ HTML è£¡æœ‰æ²’æœ‰è—å°ç£åº§æ¨™
    if not lat or not lng:
        print("âš ï¸ æœ€å¾Œæ‰‹æ®µï¼šæª¢æŸ¥åŸå§‹ HTML æ˜¯å¦æ®˜ç•™åº§æ¨™...")
        lat, lng = extract_coordinates_brute_force(html_content)

    # çµç®—
    if lat and lng:
        print(f"âœ… æœ€çµ‚é–å®š: ç·¯åº¦ {lat}, ç¶“åº¦ {lng}")
        
        data = {
            "user_id": user_id,
            "original_url": short_url,
            "name": name,
            "latitude": lat,
            "longitude": lng
        }
        
        try:
            supabase.table("ig_food_map").insert(data).execute()
            print("ğŸ‰ æˆåŠŸå„²å­˜è‡³ Supabase!")
            return True
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«å¯«å…¥å¤±æ•—: {e}")
    else:
        print("âŒ ä»»å‹™å¤±æ•—ï¼šGoogle èˆ‡ OSM çš†ç„¡æ³•å®šä½æ­¤åœ°é»ã€‚")
    
    return False

def main():
    print("ğŸš€ IG ç¾é£Ÿåœ°åœ–è§£æå™¨ V7.0 (ç›Ÿè»æ”¯æ´ç‰ˆ) å•Ÿå‹•...")
    
    if len(sys.argv) > 2:
        target_url = sys.argv[1]
        user_id = sys.argv[2]
        try:
            supabase = init_supabase()
            save_location(supabase, user_id, target_url)
        except Exception as e:
            print(f"âŒ åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}")
            sys.exit(1) 
    else:
        print("âš ï¸ ç¼ºå°‘åƒæ•¸")

if __name__ == "__main__":
    main()
