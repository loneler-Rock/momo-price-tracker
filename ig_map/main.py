# æª”æ¡ˆä½ç½®: ig_map/main.py
import os
import sys
import re
import requests
from urllib.parse import unquote
from bs4 import BeautifulSoup # å¼•å…¥å¼·å¤§çš„ç¶²é è§£æå·¥å…·

# è¨­å®šè·¯å¾‘ä»¥å¼•ç”¨ utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils.supabase_client import init_supabase

def get_url_content(short_url):
    """
    ç²å–ç¶²å€çš„æœ€çµ‚ URL å’Œ HTML å…§å®¹
    """
    try:
        # æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨ï¼Œç¢ºä¿ Google çµ¦æˆ‘å€‘å®Œæ•´çš„ç¶²é 
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
        }
        response = requests.get(short_url, headers=headers, allow_redirects=True, timeout=10)
        return response.url, response.text
    except Exception as e:
        print(f"âŒ ç¶²é è®€å–å¤±æ•—: {e}")
        return short_url, ""

def parse_dms(dms_str):
    """
    å°‡åº¦åˆ†ç§’æ ¼å¼ (25Â°03'56.9"N) è½‰æ›ç‚ºåé€²ä½
    """
    try:
        parts = re.match(r"(\d+)Â°(\d+)'([\d.]+)\"([NSEW])", dms_str)
        if parts:
            degrees = float(parts.group(1))
            minutes = float(parts.group(2))
            seconds = float(parts.group(3))
            direction = parts.group(4)
            decimal = degrees + minutes/60 + seconds/3600
            if direction in ['S', 'W']:
                decimal = -decimal
            return decimal
    except Exception as e:
        print(f"âš ï¸ DMS è½‰æ›éŒ¯èª¤: {e}")
    return None

def extract_data_from_html(html):
    """
    [V4.0 æ–°åŠŸèƒ½] å¾ç¶²é  HTML çš„ meta tag ä¸­æŒ–å‡ºåº§æ¨™å’Œåº—å
    é€™æ˜¯è™•ç†æ‰‹æ©Ÿç‰ˆé€£çµçš„é—œéµï¼
    """
    lat, lng, name = None, None, None
    try:
        soup = BeautifulSoup(html, 'html.parser')

        # 1. æŠ“å–åº—å (og:title é€šå¸¸æ˜¯ "åº—å Â· åœ°å€")
        og_title = soup.find("meta", property="og:title")
        if og_title and og_title.get("content"):
            full_title = og_title["content"]
            # Google çš„æ¨™é¡Œé€šå¸¸æ˜¯ "åº—å Â· åœ°å€"ï¼Œæˆ‘å€‘åªå–å‰é¢
            name = full_title.split('Â·')[0].strip()
            print(f"ğŸ•µï¸ é€é HTML æŠ“åˆ°åº—å: {name}")

        # 2. æŠ“å–åº§æ¨™ (å¾ og:image æŠ“å– center åƒæ•¸)
        # ç¯„ä¾‹: https://maps.google.com/.../staticmap?center=24.743,121.730&zoom=...
        og_image = soup.find("meta", property="og:image")
        if og_image and og_image.get("content"):
            image_url = og_image["content"]
            match = re.search(r'center=(-?\d+\.\d+)%2C(-?\d+\.\d+)', image_url)
            # æœ‰æ™‚å€™æ˜¯ç”¨é€—è™Ÿåˆ†éš”ï¼Œæ²’ç·¨ç¢¼
            if not match:
                match = re.search(r'center=(-?\d+\.\d+),(-?\d+\.\d+)', image_url)
                
            if match:
                lat, lng = float(match.group(1)), float(match.group(2))
                print(f"ğŸ•µï¸ é€é HTML og:image æŠ“åˆ°åº§æ¨™: {lat}, {lng}")

    except Exception as e:
        print(f"âš ï¸ HTML è§£æå¤±æ•—: {e}")
    
    return lat, lng, name

def extract_name_from_url(url):
    """
    å¾ç¶²å€ä¸­æŒ–æ˜åº—å (å‚™ç”¨)
    """
    try:
        decoded_url = unquote(url)
        match = re.search(r'/place/([^/]+)/', decoded_url)
        if match:
            return match.group(1).replace('+', ' ')
    except:
        pass
    return None

def extract_coordinates_from_url(url):
    """
    å¾ç¶²å€è§£æç¶“ç·¯åº¦ (å‚™ç”¨)
    """
    decoded_url = unquote(url)
    
    # Pattern 1: @lat,long
    match = re.search(r'@(-?\d+\.\d+),(-?\d+\.\d+)', decoded_url)
    if match: return float(match.group(1)), float(match.group(2))
        
    # Pattern 2: q=lat,long
    match = re.search(r'q=(-?\d+\.\d+),(-?\d+\.\d+)', decoded_url)
    if match: return float(match.group(1)), float(match.group(2))
        
    # Pattern 3: !3d...!4d...
    match = re.search(r'!3d(-?\d+\.\d+)!4d(-?\d+\.\d+)', decoded_url)
    if match: return float(match.group(1)), float(match.group(2))

    # Pattern 4: DMS æ ¼å¼
    try:
        lat_match = re.search(r'(\d+Â°\d+\'[\d.]+"[NS])', decoded_url)
        lng_match = re.search(r'(\d+Â°\d+\'[\d.]+"[EW])', decoded_url)
        if lat_match and lng_match:
            return parse_dms(lat_match.group(1)), parse_dms(lng_match.group(1))
    except:
        pass

    return None, None

def save_location(supabase, user_id, short_url):
    print(f"ğŸ” æ­£åœ¨è§£æ: {short_url} ...")
    
    # 1. å–å¾—æœ€çµ‚ç¶²å€èˆ‡ç¶²é å…§å®¹ (é€™æ˜¯ V4.0 çš„æ ¸å¿ƒ)
    final_url, html_content = get_url_content(short_url)
    print(f"â¡ï¸ æœ€çµ‚ç¶²å€: {final_url[:80]}...") 
    
    # 2. å…ˆå˜—è©¦å¾ HTML (çˆ¬èŸ²) ç²å–è³‡æ–™ -> é€™æ˜¯æœ€æº–çš„
    lat, lng, html_name = extract_data_from_html(html_content)
    
    # 3. å¦‚æœ HTML æ²’æŠ“åˆ°ï¼Œå†ç”¨èˆŠæ–¹æ³•å¾ URL ç®—
    if not lat or not lng:
        print("âš ï¸ HTML å…§ç„¡åº§æ¨™ï¼Œå˜—è©¦å¾ç¶²å€è§£æ...")
        lat, lng = extract_coordinates_from_url(final_url)
        
    # åº—åé‚è¼¯ï¼šå„ªå…ˆç”¨ HTML æŠ“åˆ°çš„ä¸­æ–‡åï¼Œæ²’æœ‰æ‰ç”¨ç¶²å€è§£ç¢¼
    shop_name = html_name if html_name else extract_name_from_url(final_url)
    if not shop_name:
        shop_name = "æœªå‘½ååœ°é»"

    print(f"ğŸ·ï¸ æœ€çµ‚åˆ¤å®šåº—å: {shop_name}")
    
    if lat and lng:
        print(f"âœ… æˆåŠŸé–å®š: ç·¯åº¦ {lat}, ç¶“åº¦ {lng}")
        
        data = {
            "user_id": user_id,
            "original_url": short_url,
            "name": shop_name,
            "latitude": lat,
            "longitude": lng
        }
        
        try:
            supabase.table("ig_food_map").insert(data).execute()
            print("ğŸ‰ æˆåŠŸå„²å­˜è‡³ Supabase!")
            return True
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«å¯«å…¥å¤±æ•—: {e}")
    else:
        print("âš ï¸ ç„¡æ³•è§£æå‡ºåº§æ¨™ (URLèˆ‡HTMLçš†å¤±æ•—)ã€‚")
    
    return False

def main():
    print("ğŸš€ IG ç¾é£Ÿåœ°åœ–è§£æå™¨ V4.0 (çˆ¬èŸ²å¼·æ”»ç‰ˆ) å•Ÿå‹•...")
    
    if len(sys.argv) > 2:
        target_url = sys.argv[1]
        user_id = sys.argv[2]
        
        print(f"æ”¶åˆ°æŒ‡ä»¤ï¼\nä½¿ç”¨è€…: {user_id}\nç¶²å€: {target_url}")
        
        try:
            supabase = init_supabase()
            save_location(supabase, user_id, target_url)
        except Exception as e:
            print(f"âŒ åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}")
            sys.exit(1) 
    else:
        print("âš ï¸ ç¼ºå°‘åƒæ•¸ï¼Œè«‹é€é GitHub Actions åŸ·è¡Œã€‚")

if __name__ == "__main__":
    main()
